{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxboard\n",
      "  Using cached https://files.pythonhosted.org/packages/3e/e4/57f6884c39b471c8fd446dc59998045ceab1c9ebe4a6091c953d97a60934/mxboard-0.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from mxboard) (5.0.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from mxboard) (1.14.3)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from mxboard) (3.5.2)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from mxboard) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from protobuf>=3.0.0->mxboard) (39.1.0)\n",
      "Installing collected packages: mxboard\n",
      "Successfully installed mxboard-0.1.0\n",
      "Requirement already satisfied: gluonnlp in /home/ubuntu/anaconda3/lib/python3.6/site-packages (0.3.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from gluonnlp) (1.14.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxboard\n",
    "!pip install gluonnlp --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training a Sentiment Analysis Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective\n",
    "- Using the preprocessing we performed previously we can then use pretrained models to train a sentiment analysis on IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)\n",
    "max_gpus = 8\n",
    "num_gpus = min(max_gpus, mx.context.num_gpus())\n",
    "dropout = 0\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "learning_rate = 0.005 * num_gpus\n",
    "batch_size = 16 * num_gpus\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.2\n",
    "epochs = 1\n",
    "grad_clip = None\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building the Model\n",
    "![](https://gluon-nlp.mxnet.io/_images/samodel-v3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "######################################### Model #########################################\n",
    "class SentimentNet(gluon.Block):\n",
    "    def __init__(self, embedding_block, encoder_block, dropout,\n",
    "                 prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = embedding_block\n",
    "            self.encoder = encoder_block\n",
    "            self.out_layer = gluon.nn.HybridSequential()\n",
    "            with self.out_layer.name_scope():\n",
    "                self.out_layer.add(gluon.nn.Dropout(dropout))\n",
    "                self.out_layer.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        encoded = self.encoder(nd.Dropout(self.embedding(data),\n",
    "                                          0.2, axes=(0,)))  # Shape(T, N, C)\n",
    "        # Zero out the values with position exceeding the valid length.\n",
    "        masked_encoded = nd.SequenceMask(encoded,\n",
    "                                         sequence_length=valid_length,\n",
    "                                         use_sequence_length=True)\n",
    "        agg_state = nd.broadcast_div(nd.sum(masked_encoded, axis=0),\n",
    "                                     nd.expand_dims(valid_length, axis=1))\n",
    "        out = self.out_layer(agg_state)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ```python mx.nd.array.SequenceMask(data, sequence_length, use_sequence_length, value=, out, name, **kwargs)```:\n",
    "Sets all elements outside the sequence to a constant value.\n",
    "- ```python mxnet.ndarray.broadcast_div(lhs, rhs, out, name, **kwargs)```: Returns element-wise division of the input arrays with broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py:420: UserWarning: load_params is deprecated. Please use load_parameters.\n",
      "  warnings.warn(\"load_params is deprecated. Please use load_parameters.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################### MODEL #######################\n",
      "\n",
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2)\n",
      "  (out_layer): HybridSequential(\n",
      "    (0): Dropout(p = 0, axes=())\n",
      "    (1): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=ctx,\n",
    "                                      dropout=dropout)\n",
    "net = SentimentNet(embedding_block=lm_model.embedding,\n",
    "                   encoder_block=lm_model.encoder,\n",
    "                   dropout=dropout)\n",
    "net.out_layer.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "print(\"\\n####################### MODEL #######################\\n\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize using spaCy...\n"
     ]
    }
   ],
   "source": [
    "######################################### Data #########################################\n",
    "# train_dataset and test_dataset are both SimpleDataset objects,\n",
    "# which is a wrapper for lists and arrays.\n",
    "train_dataset, test_dataset = [nlp.data.IMDB(segment=segment)\n",
    "                               for segment in ('train', 'test')]\n",
    "print(\"Tokenize using spaCy...\")\n",
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "# length_clip takes as input a list and outputs a list with maximum length 500.\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    data, label = x\n",
    "    # In the labeled train/test sets, a negative review has a score <= 4\n",
    "    # out of 10, and a positive review has a score >= 7 out of 10. Thus\n",
    "    # reviews with more neutral ratings are not included in the train/test\n",
    "    # sets. We labeled a negative review whose score <= 4 as 0, and a\n",
    "    # positive review whose score >= 7 as 1. As the neural ratings are not\n",
    "    # included in the datasets, we can simply use 5 as our threshold.\n",
    "    label = int(label > 5)\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    data = vocab[length_clip(tokenizer(data))]\n",
    "    return data, label, float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_length(x):\n",
    "    return x[2]\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
    "          .format(end - start, len(dataset)))\n",
    "    return dataset, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=6.09s, #Sentences=25000\n",
      "Done! Tokenizing Time=6.08s, #Sentences=25000\n",
      "\n",
      "####################### DATA #######################\n",
      "\n",
      "([0, 1409, 24, 9, 13139, 1717, 4, 61, 1117, 27, 2, 152, 62, 16, 98, 64, 2170, 80, 391, 180, 3, 91, 16, 11, 21820, 11, 4, 1196, 1230, 104, 7, 2, 2632, 13392, 409, 706, 8, 1907, 15, 0, 1409, 17, 7922, 24, 217, 3430, 8, 2564, 84, 24, 11, 21820, 11, 4, 13, 20391, 8, 3660, 7863, 3, 2, 0, 914, 55, 110, 594, 455, 109, 38, 18190, 5232, 57, 0, 3, 2, 0, 5, 2, 1147, 1309, 3, 68, 16731, 706, 5, 2, 1206, 70, 2561, 6, 38, 914, 4, 274, 70, 693, 2, 138, 7, 33, 9, 1838, 5718, 1650, 8, 3685, 197, 2, 391, 3, 70, 1297, 3163, 0, 27, 0, 1409, 4, 76, 3473, 220, 43, 0, 43, 70, 1921, 1914, 8, 0, 49, 5, 1903, 5232, 4, 0, 43, 17622, 8, 0, 1409, 4, 70, 9041, 15, 125, 3233, 5, 750, 507, 1229, 15, 0, 1409, 24, 754, 0, 4, 2493, 9, 18214, 15, 29, 24, 0, 365], 1, 165.0)\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)\n",
    "print(\"\\n####################### DATA #######################\\n\")\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######################################### Train #########################################\n",
    "def evaluate(net, dataloader, ctx):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    print('Begin Testing...')\n",
    "    for i, (data, label, valid_length) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(ctx))\n",
    "        valid_length = valid_length.as_in_context(ctx).astype(np.float32)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data, valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader),\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "def train(net, ctx, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(),\n",
    "                            'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack(dtype=np.float32),\n",
    "                                          nlp.data.batchify.Stack(dtype=np.float32))\n",
    "\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(train_data_lengths,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        num_buckets=bucket_num,\n",
    "                                                        ratio=bucket_ratio,\n",
    "                                                        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batch_sampler=batch_sampler,\n",
    "                                             batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            batchify_fn=batchify_fn)\n",
    "    parameters = net.collect_params().values()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "def train(net, ctx, epochs):\n",
    "...\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "    ...\n",
    "        for i, (data, label, length) in enumerate(train_dataloader):\n",
    "            if data.shape[0] > len(ctx):\n",
    "                # Multi-gpu training.\n",
    "                data_list, label_list, length_list \\\n",
    "                    = [gluon.utils.split_and_load(x, ctx, batch_axis=0, even_split=False)\n",
    "                                                           for x in [data, label, length]]\n",
    "            else:\n",
    "                data_list = [data.as_in_context(ctx[0])]\n",
    "                label_list = [label.as_in_context(ctx[0])]\n",
    "                length_list = [length.as_in_context(ctx[0])]\n",
    "            ...\n",
    "            for data, label, valid_length in zip(data_list, label_list, length_list):\n",
    "                valid_length = valid_length\n",
    "                with autograd.record():\n",
    "                    output = net(data.T, valid_length)\n",
    "                    L = L + loss(output, label).mean().as_in_context(ctx[0])\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm([p.grad(x.context)\n",
    "                                              for p in parameters for x in data_list], grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, ctx, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(),\n",
    "                            'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack(dtype=np.float32),\n",
    "                                          nlp.data.batchify.Stack(dtype=np.float32))\n",
    "\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(train_data_lengths,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        num_buckets=bucket_num,\n",
    "                                                        ratio=bucket_ratio,\n",
    "                                                        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batch_sampler=batch_sampler,\n",
    "                                             batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            batchify_fn=batchify_fn)\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (data, label, length) in enumerate(train_dataloader):\n",
    "            if data.shape[0] > len(ctx):\n",
    "                # Multi-gpu training.\n",
    "                data_list, label_list, length_list \\\n",
    "                    = [gluon.utils.split_and_load(x,\n",
    "                                                  ctx,\n",
    "                                                  batch_axis=0,\n",
    "                                                  even_split=False)\n",
    "                       for x in [data, label, length]]\n",
    "            else:\n",
    "                data_list = [data.as_in_context(ctx[0])]\n",
    "                label_list = [label.as_in_context(ctx[0])]\n",
    "                length_list = [length.as_in_context(ctx[0])]\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            for data, label, valid_length in zip(data_list, label_list, length_list):\n",
    "                valid_length = valid_length\n",
    "                with autograd.record():\n",
    "                    output = net(data.T, valid_length)\n",
    "                    L = L + loss(output, label).mean().as_in_context(ctx[0])\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm([p.grad(x.context)\n",
    "                                              for p in parameters for x in data_list],\n",
    "                                             grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print('[Epoch {} Batch {}/{}] elapsed {:.2f} s, \\\n",
    "                      avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, len(train_dataloader),\n",
    "                           time.time() - start_log_interval_time,\n",
    "                           log_interval_L / log_interval_sent_num,\n",
    "                           log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, ctx[0])\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, \\\n",
    "        test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "            epoch, epoch_L / epoch_sent_num,\n",
    "            test_acc, test_avg_L, epoch_wc / 1000 /\n",
    "                   (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=25000, batch_num=392\n",
      "  key=[59, 108, 157, 206, 255, 304, 353, 402, 451, 500]\n",
      "  cnt=[590, 1999, 5092, 5102, 3038, 2085, 1477, 1165, 870, 3582]\n",
      "  batch_size=[108, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
      "[Epoch 0 Batch 100/392] elapsed 5.31 s,                       avg loss 0.008419, throughput 300.98K wps\n",
      "[Epoch 0 Batch 200/392] elapsed 4.81 s,                       avg loss 0.005679, throughput 332.25K wps\n",
      "[Epoch 0 Batch 300/392] elapsed 5.00 s,                       avg loss 0.005286, throughput 295.92K wps\n",
      "Begin Testing...\n",
      "[Batch 100/391] elapsed 6.30 s\n",
      "[Batch 200/391] elapsed 6.34 s\n",
      "[Batch 300/391] elapsed 6.49 s\n",
      "[Epoch 0] train avg loss 0.006042, test acc 0.83,         test avg loss 0.331884, throughput 317.15K wps\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "train(net, ctx, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "#Train\n",
    "train(net, ctx, epochs)\n",
    "```\n",
    "![](./training_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
